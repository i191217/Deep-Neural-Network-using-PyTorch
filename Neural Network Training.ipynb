{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "OtkpkpA3v30T"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhL5gtUGFcah"
   },
   "source": [
    "# Wine Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bYFmr7V2DX-z"
   },
   "outputs": [],
   "source": [
    "class WineDatset(Dataset):\n",
    "  def __init__(self, directory):\n",
    "    wineDataset = pd.read_csv(directory)\n",
    "    features=wineDataset.iloc[:, 0:11]\n",
    "    labels=wineDataset.iloc[:, 11:12]\n",
    "    \n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    sample = {'Features': self.features.iloc[idx], 'Labels': self.labels.iloc[idx]}\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Oc4VEgK6EDGG"
   },
   "outputs": [],
   "source": [
    "wineDataset = WineDatset('/content/winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "krR7ly0REZ55"
   },
   "outputs": [],
   "source": [
    "test_pct = 0.2\n",
    "test_size = int(len(wineDataset)*test_pct)\n",
    "train_size = len(wineDataset) - test_size\n",
    "train_ds, test_ds = torch.utils.data.random_split(wineDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAJLjZqfFxWJ"
   },
   "source": [
    "#Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "849Ntr5BEpnR"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 5, shuffle=True, num_workers=2,collate_fn=lambda x: x,pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, 5, num_workers=2, collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pM_KNY1bFndI",
    "outputId": "5c3b2ce3-3f29-465e-befa-a24c9ad5bf1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fixed acidity             7.70000\n",
      "volatile acidity          0.58000\n",
      "citric acid               0.10000\n",
      "residual sugar            1.80000\n",
      "chlorides                 0.10200\n",
      "free sulfur dioxide      28.00000\n",
      "total sulfur dioxide    109.00000\n",
      "density                   0.99565\n",
      "pH                        3.08000\n",
      "sulphates                 0.49000\n",
      "alcohol                   9.80000\n",
      "Name: 242, dtype: float64, fixed acidity            5.60000\n",
      "volatile acidity         0.91500\n",
      "citric acid              0.00000\n",
      "residual sugar           2.10000\n",
      "chlorides                0.04100\n",
      "free sulfur dioxide     17.00000\n",
      "total sulfur dioxide    78.00000\n",
      "density                  0.99346\n",
      "pH                       3.68000\n",
      "sulphates                0.73000\n",
      "alcohol                 11.40000\n",
      "Name: 1178, dtype: float64, fixed acidity             8.5000\n",
      "volatile acidity          0.2800\n",
      "citric acid               0.5600\n",
      "residual sugar            1.8000\n",
      "chlorides                 0.0920\n",
      "free sulfur dioxide      35.0000\n",
      "total sulfur dioxide    103.0000\n",
      "density                   0.9969\n",
      "pH                        3.3000\n",
      "sulphates                 0.7500\n",
      "alcohol                  10.5000\n",
      "Name: 16, dtype: float64, fixed acidity           10.3000\n",
      "volatile acidity         0.5300\n",
      "citric acid              0.4800\n",
      "residual sugar           2.5000\n",
      "chlorides                0.0630\n",
      "free sulfur dioxide      6.0000\n",
      "total sulfur dioxide    25.0000\n",
      "density                  0.9998\n",
      "pH                       3.1200\n",
      "sulphates                0.5900\n",
      "alcohol                  9.3000\n",
      "Name: 310, dtype: float64, fixed acidity             7.4000\n",
      "volatile acidity          0.6000\n",
      "citric acid               0.2600\n",
      "residual sugar            7.3000\n",
      "chlorides                 0.0700\n",
      "free sulfur dioxide      36.0000\n",
      "total sulfur dioxide    121.0000\n",
      "density                   0.9982\n",
      "pH                        3.3700\n",
      "sulphates                 0.4900\n",
      "alcohol                   9.4000\n",
      "Name: 163, dtype: float64]\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dl):\n",
    "  # observe 1st batch and stop.\n",
    "  if i_batch == 1:\n",
    "      batch = [sample_batched[i]['Features'] for i in range(0,len(sample_batched))]\n",
    "      print(batch)\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e8zZRGYNNGK",
    "outputId": "0a0018bc-f620-4784-9d0d-b8e3cdb8c1da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtdjZjSaM3lh"
   },
   "source": [
    "#Neural Network Class with 3 FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "IRKaLiHlL-Vh"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(11)\n",
    "        self.fc1 = nn.Linear(11, 20)\n",
    "        self.fc2 = nn.Linear(20, 30)\n",
    "        self.fc3 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "TaWkfAiNRBhT"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(actual, predicted):\n",
    "\tsum_square_error = 0.0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tsum_square_error += (actual[i] - predicted[i])**2.0\n",
    "\tmean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "\treturn mean_square_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "95OwkEV1HA_H"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkE_joInRCLe",
    "outputId": "80ceab93-de76-4701-a3f0-12f5251addfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 4.592\n",
      "[1,   200] loss: 0.368\n",
      "[2,   100] loss: 0.320\n",
      "[2,   200] loss: 0.306\n",
      "[3,   100] loss: 0.362\n",
      "[3,   200] loss: 0.285\n",
      "[4,   100] loss: 0.383\n",
      "[4,   200] loss: 0.273\n",
      "[5,   100] loss: 0.315\n",
      "[5,   200] loss: 0.346\n",
      "[6,   100] loss: 0.315\n",
      "[6,   200] loss: 0.316\n",
      "[7,   100] loss: 0.317\n",
      "[7,   200] loss: 0.320\n",
      "[8,   100] loss: 0.349\n",
      "[8,   200] loss: 0.308\n",
      "[9,   100] loss: 0.328\n",
      "[9,   200] loss: 0.329\n",
      "[10,   100] loss: 0.311\n",
      "[10,   200] loss: 0.328\n",
      "[11,   100] loss: 0.310\n",
      "[11,   200] loss: 0.340\n",
      "[12,   100] loss: 0.324\n",
      "[12,   200] loss: 0.314\n",
      "[13,   100] loss: 0.335\n",
      "[13,   200] loss: 0.306\n",
      "[14,   100] loss: 0.328\n",
      "[14,   200] loss: 0.315\n",
      "[15,   100] loss: 0.367\n",
      "[15,   200] loss: 0.285\n",
      "[16,   100] loss: 0.315\n",
      "[16,   200] loss: 0.329\n",
      "[17,   100] loss: 0.312\n",
      "[17,   200] loss: 0.344\n",
      "[18,   100] loss: 0.349\n",
      "[18,   200] loss: 0.311\n",
      "[19,   100] loss: 0.359\n",
      "[19,   200] loss: 0.320\n",
      "[20,   100] loss: 0.308\n",
      "[20,   200] loss: 0.323\n",
      "[21,   100] loss: 0.320\n",
      "[21,   200] loss: 0.320\n",
      "[22,   100] loss: 0.329\n",
      "[22,   200] loss: 0.366\n",
      "[23,   100] loss: 0.329\n",
      "[23,   200] loss: 0.320\n",
      "[24,   100] loss: 0.317\n",
      "[24,   200] loss: 0.346\n",
      "[25,   100] loss: 0.315\n",
      "[25,   200] loss: 0.339\n",
      "[26,   100] loss: 0.328\n",
      "[26,   200] loss: 0.330\n",
      "[27,   100] loss: 0.319\n",
      "[27,   200] loss: 0.367\n",
      "[28,   100] loss: 0.323\n",
      "[28,   200] loss: 0.352\n",
      "[29,   100] loss: 0.306\n",
      "[29,   200] loss: 0.347\n",
      "[30,   100] loss: 0.325\n",
      "[30,   200] loss: 0.338\n",
      "[31,   100] loss: 0.322\n",
      "[31,   200] loss: 0.331\n",
      "[32,   100] loss: 0.293\n",
      "[32,   200] loss: 0.359\n",
      "[33,   100] loss: 0.334\n",
      "[33,   200] loss: 0.306\n",
      "[34,   100] loss: 0.325\n",
      "[34,   200] loss: 0.340\n",
      "[35,   100] loss: 0.331\n",
      "[35,   200] loss: 0.307\n",
      "[36,   100] loss: 0.317\n",
      "[36,   200] loss: 0.330\n",
      "[37,   100] loss: 0.327\n",
      "[37,   200] loss: 0.318\n",
      "[38,   100] loss: 0.323\n",
      "[38,   200] loss: 0.334\n",
      "[39,   100] loss: 0.347\n",
      "[39,   200] loss: 0.322\n",
      "[40,   100] loss: 0.342\n",
      "[40,   200] loss: 0.333\n",
      "[41,   100] loss: 0.326\n",
      "[41,   200] loss: 0.350\n",
      "[42,   100] loss: 0.319\n",
      "[42,   200] loss: 0.320\n",
      "[43,   100] loss: 0.308\n",
      "[43,   200] loss: 0.341\n",
      "[44,   100] loss: 0.330\n",
      "[44,   200] loss: 0.341\n",
      "[45,   100] loss: 0.320\n",
      "[45,   200] loss: 0.333\n",
      "[46,   100] loss: 0.336\n",
      "[46,   200] loss: 0.314\n",
      "[47,   100] loss: 0.337\n",
      "[47,   200] loss: 0.324\n",
      "[48,   100] loss: 0.314\n",
      "[48,   200] loss: 0.314\n",
      "[49,   100] loss: 0.326\n",
      "[49,   200] loss: 0.332\n",
      "[50,   100] loss: 0.341\n",
      "[50,   200] loss: 0.332\n",
      "[51,   100] loss: 0.322\n",
      "[51,   200] loss: 0.342\n",
      "[52,   100] loss: 0.350\n",
      "[52,   200] loss: 0.301\n",
      "[53,   100] loss: 0.347\n",
      "[53,   200] loss: 0.318\n",
      "[54,   100] loss: 0.345\n",
      "[54,   200] loss: 0.322\n",
      "[55,   100] loss: 0.314\n",
      "[55,   200] loss: 0.330\n",
      "[56,   100] loss: 0.316\n",
      "[56,   200] loss: 0.344\n",
      "[57,   100] loss: 0.318\n",
      "[57,   200] loss: 0.305\n",
      "[58,   100] loss: 0.329\n",
      "[58,   200] loss: 0.318\n",
      "[59,   100] loss: 0.310\n",
      "[59,   200] loss: 0.315\n",
      "[60,   100] loss: 0.308\n",
      "[60,   200] loss: 0.318\n",
      "[61,   100] loss: 0.339\n",
      "[61,   200] loss: 0.336\n",
      "[62,   100] loss: 0.327\n",
      "[62,   200] loss: 0.334\n",
      "[63,   100] loss: 0.321\n",
      "[63,   200] loss: 0.328\n",
      "[64,   100] loss: 0.291\n",
      "[64,   200] loss: 0.394\n",
      "[65,   100] loss: 0.332\n",
      "[65,   200] loss: 0.337\n",
      "[66,   100] loss: 0.316\n",
      "[66,   200] loss: 0.347\n",
      "[67,   100] loss: 0.312\n",
      "[67,   200] loss: 0.338\n",
      "[68,   100] loss: 0.334\n",
      "[68,   200] loss: 0.323\n",
      "[69,   100] loss: 0.366\n",
      "[69,   200] loss: 0.306\n",
      "[70,   100] loss: 0.320\n",
      "[70,   200] loss: 0.326\n",
      "[71,   100] loss: 0.343\n",
      "[71,   200] loss: 0.317\n",
      "[72,   100] loss: 0.326\n",
      "[72,   200] loss: 0.326\n",
      "[73,   100] loss: 0.329\n",
      "[73,   200] loss: 0.316\n",
      "[74,   100] loss: 0.320\n",
      "[74,   200] loss: 0.320\n",
      "[75,   100] loss: 0.321\n",
      "[75,   200] loss: 0.346\n",
      "[76,   100] loss: 0.301\n",
      "[76,   200] loss: 0.351\n",
      "[77,   100] loss: 0.337\n",
      "[77,   200] loss: 0.309\n",
      "[78,   100] loss: 0.328\n",
      "[78,   200] loss: 0.318\n",
      "[79,   100] loss: 0.326\n",
      "[79,   200] loss: 0.339\n",
      "[80,   100] loss: 0.346\n",
      "[80,   200] loss: 0.305\n",
      "[81,   100] loss: 0.343\n",
      "[81,   200] loss: 0.325\n",
      "[82,   100] loss: 0.308\n",
      "[82,   200] loss: 0.317\n",
      "[83,   100] loss: 0.329\n",
      "[83,   200] loss: 0.324\n",
      "[84,   100] loss: 0.326\n",
      "[84,   200] loss: 0.327\n",
      "[85,   100] loss: 0.331\n",
      "[85,   200] loss: 0.321\n",
      "[86,   100] loss: 0.301\n",
      "[86,   200] loss: 0.331\n",
      "[87,   100] loss: 0.369\n",
      "[87,   200] loss: 0.287\n",
      "[88,   100] loss: 0.348\n",
      "[88,   200] loss: 0.323\n",
      "[89,   100] loss: 0.332\n",
      "[89,   200] loss: 0.338\n",
      "[90,   100] loss: 0.317\n",
      "[90,   200] loss: 0.340\n",
      "[91,   100] loss: 0.323\n",
      "[91,   200] loss: 0.327\n",
      "[92,   100] loss: 0.329\n",
      "[92,   200] loss: 0.321\n",
      "[93,   100] loss: 0.330\n",
      "[93,   200] loss: 0.332\n",
      "[94,   100] loss: 0.309\n",
      "[94,   200] loss: 0.322\n",
      "[95,   100] loss: 0.324\n",
      "[95,   200] loss: 0.348\n",
      "[96,   100] loss: 0.309\n",
      "[96,   200] loss: 0.342\n",
      "[97,   100] loss: 0.333\n",
      "[97,   200] loss: 0.328\n",
      "[98,   100] loss: 0.312\n",
      "[98,   200] loss: 0.340\n",
      "[99,   100] loss: 0.320\n",
      "[99,   200] loss: 0.324\n",
      "[100,   100] loss: 0.326\n",
      "[100,   200] loss: 0.327\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        features, labels = [data[i]['Features'] for i in range(0,len(data))], [data[i]['Labels'] for i in range(0,len(data))]\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(torch.as_tensor(features).float())\n",
    "        loss = mean_squared_error(outputs, torch.as_tensor(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "lOkKxH4sRgLJ"
   },
   "outputs": [],
   "source": [
    "PATH = 'wine-model.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZAF_W7LWggX",
    "outputId": "8dc309f8-2718-4e65-aef5-99496d1f7b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "sum1 = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dl):\n",
    "        features, labels = [data[i]['Features'] for i in range(0,len(data))], [data[i]['Labels'] for i in range(0,len(data))]\n",
    "        outputs = net(torch.as_tensor(features).float())\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        labels = torch.as_tensor(labels)\n",
    "        sum1 += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "\n",
    "print('Accuracy: %d %%' % (100 * correct / sum1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnNMyRZhWyxe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Wine Dataset Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
